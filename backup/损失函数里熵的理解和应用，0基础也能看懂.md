熵是信息论中的一个概念，用于衡量一个随机变量不确定性的量度。在概率论和统计学中，离散随机变量的熵是一个度量其概率分布中不确定性或信息内容的函数。

对于一个离散随机变量 \( X \) ，它可能的取值为 \( x_1, x_2, ..., x_n \) ，相应的概率为 \( P(X=x_i) = p_i \) ，其中 \( i = 1, 2, ..., n \) 并且 \( \sum_{i=1}^{n} p_i = 1 \) 。这个随机变量的熵 \( H(X) \) 定义为：

\[ H(X) = -\sum_{i=1}^{n} p_i \log_b(p_i) \]

这里，\( \log_b \) 表示以 \( b \) 为底的对数。在信息论中，通常使用2作为底数，此时熵的单位是比特（bit）；使用自然对数（底数为 \( e \) ）时，单位是纳特（nat）；使用10为底数时，单位是哈特利（hartley）。

### 具体例子

假设我们有一个离散随机变量 \( X \) ，它代表抛掷一枚不公平的硬币的结果，硬币正面朝上（记为 \( H \) ）的概率是 \( p \) ，反面朝上（记为 \( T \) ）的概率是 \( 1 - p \) 。

1. 当 \( p = 0.5 \) 时，硬币是公平的，熵最大，计算如下：
   \[ H(X) = - (0.5 \log_2(0.5) + 0.5 \log_2(0.5)) \]
   \[ H(X) = - (0.5 \times -1 + 0.5 \times -1) \]
   \[ H(X) = 1 \text{ bit} \]

2. 当 \( p = 0 \) 或 \( p = 1 \) 时，硬币的结果完全确定，没有不确定性，熵为0：
   \[ H(X) = - (1 \log_2(1) + 0 \log_2(0)) = 0 \text{ bit} \]
   注意：在实际计算中，尽管对数函数在0上没有定义，\( 0 \log_2(0) \) 通常被认为是0，因为当 \( p_i = 0 \) 时，随机变量不存在，当然对熵也没有影响。

3. 当 \( p = 0.9 \) 时，硬币几乎总是正面朝上，熵较小，计算如下：
   \[ H(X) = - (0.9 \log_2(0.9) + 0.1 \log_2(0.1)) \]
   \[ H(X) \approx - (0.9 \times -0.152 + 0.1 \times -3.32) \]
   \[ H(X) \approx 0.47 \text{ bits} \]

这个例子展示了熵如何量化随机变量的不确定性：当结果越不确定（即概率分布越均匀），熵就越大；当结果越确定（即某个结果的概率接近1），熵就越小。
